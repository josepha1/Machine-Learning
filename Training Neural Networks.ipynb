{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n",
    "\n",
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from bottom to top here. We pass the input $x$ through a linear transformation $L_1$ with weights $W_1$ and biases $b_1$. The output then goes through the sigmoid operation $S$ and another linear transformation $L_2$. Finally we calculate the loss $\\ell$. We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Note:** I'm glossing over a few details here that require some knowledge of vector calculus, but they aren't necessary to understand what's going on.\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 2.1.0\n",
      "\t• tf.keras version: 2.2.4-tf\n",
      "\t• GPU device not found. Running on CPU\n"
     ]
    }
   ],
   "source": [
    "print('Using:')\n",
    "print(f'\\t\\u2022 TensorFlow version: {tf.__version__}')\n",
    "print(f'\\t\\u2022 tf.keras version: {tf.keras.__version__}')\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, dataset_info = tfds.load('mnist', split = 'train', as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "num_training_examples = dataset_info.splits['train'].num_examples\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples // 4).batch(batch_size).map(normalize).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Model Ready For Training\n",
    "\n",
    "Before we can train our model we need to set the parameters we are going to use to train it. We can configure our model for training using the `.compile` method. The main parameters we need to specify in the `.compile` method are:\n",
    "\n",
    "* **Optimizer:** The algorithm that we'll use to update the weights of our model during training. Throughout these lessons we will use the [`adam`](http://arxiv.org/abs/1412.6980) optimizer. Adam is an optimization of the stochastic gradient descent algorithm. For a full list of the optimizers available in `tf.keras` check out the [optimizers documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers#classes).\n",
    "\n",
    "\n",
    "* **Loss Function:** The loss function we are going to use during training to measure the difference between the true labels of the images in your dataset and the predictions made by your model. In this lesson we will use the `sparse_categorical_crossentropy` loss function. We use the `sparse_categorical_crossentropy` loss function when our dataset has labels that are integers, and the `categorical_crossentropy` loss function when our dataset has one-hot encoded labels. For a full list of the loss functions available in `tf.keras` check out the [losses documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses#classes).\n",
    "\n",
    "\n",
    "* **Metrics:** A list of metrics to be evaluated by the model during training. Throughout these lessons we will measure the `accuracy` of our model. The `accuracy` calculates how often our model's predictions match the true labels of the images in our dataset. For a full list of the metrics available in `tf.keras` check out the [metrics documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics#classes).\n",
    "\n",
    "These are the main parameters we are going to set throughout these lessons. You can check out all the other configuration parameters in the [TensorFlow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 2.3914 - accuracy: 0.06 - 0s 206us/sample - loss: 2.3451 - accuracy: 0.0781\n",
      "\n",
      "Loss before training: 2.345\n",
      "Accuracy before training: 7.812%\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "\n",
    "print('\\nLoss before training: {:,.3f}'.format(loss))\n",
    "print('Accuracy before training: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now let's train our model by using all the images in our training set. Some nomenclature, one pass through the entire dataset is called an *epoch*. To train our model for a given number of epochs we use the `.fit` method, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    173/Unknown - 2s 2s/step - loss: 2.2893 - accuracy: 0.09 - 2s 1s/step - loss: 2.2688 - accuracy: 0.12 - 2s 809ms/step - loss: 2.2308 - accuracy: 0.171 - 2s 611ms/step - loss: 2.2064 - accuracy: 0.210 - 2s 493ms/step - loss: 2.1599 - accuracy: 0.271 - 2s 415ms/step - loss: 2.1368 - accuracy: 0.289 - 3s 358ms/step - loss: 2.1058 - accuracy: 0.323 - 3s 314ms/step - loss: 2.0804 - accuracy: 0.337 - 3s 285ms/step - loss: 2.0542 - accuracy: 0.366 - 3s 258ms/step - loss: 2.0205 - accuracy: 0.387 - 3s 235ms/step - loss: 1.9809 - accuracy: 0.411 - 3s 216ms/step - loss: 1.9535 - accuracy: 0.427 - 3s 201ms/step - loss: 1.9244 - accuracy: 0.442 - 3s 187ms/step - loss: 1.8884 - accuracy: 0.458 - 3s 176ms/step - loss: 1.8551 - accuracy: 0.474 - 3s 166ms/step - loss: 1.8175 - accuracy: 0.494 - 3s 159ms/step - loss: 1.7884 - accuracy: 0.508 - 3s 151ms/step - loss: 1.7534 - accuracy: 0.526 - 3s 144ms/step - loss: 1.7229 - accuracy: 0.534 - 3s 137ms/step - loss: 1.6874 - accuracy: 0.546 - 3s 131ms/step - loss: 1.6654 - accuracy: 0.553 - 3s 126ms/step - loss: 1.6426 - accuracy: 0.557 - 3s 121ms/step - loss: 1.6174 - accuracy: 0.563 - 3s 117ms/step - loss: 1.5961 - accuracy: 0.567 - 3s 113ms/step - loss: 1.5732 - accuracy: 0.573 - 3s 109ms/step - loss: 1.5499 - accuracy: 0.578 - 3s 105ms/step - loss: 1.5248 - accuracy: 0.587 - 3s 102ms/step - loss: 1.5021 - accuracy: 0.593 - 3s 99ms/step - loss: 1.4810 - accuracy: 0.598 - 3s 96ms/step - loss: 1.4577 - accuracy: 0.60 - 3s 93ms/step - loss: 1.4370 - accuracy: 0.61 - 3s 90ms/step - loss: 1.4193 - accuracy: 0.61 - 3s 88ms/step - loss: 1.3992 - accuracy: 0.62 - 3s 87ms/step - loss: 1.3777 - accuracy: 0.62 - 3s 85ms/step - loss: 1.3576 - accuracy: 0.63 - 3s 83ms/step - loss: 1.3400 - accuracy: 0.63 - 3s 81ms/step - loss: 1.3209 - accuracy: 0.64 - 3s 79ms/step - loss: 1.2994 - accuracy: 0.65 - 3s 77ms/step - loss: 1.2831 - accuracy: 0.65 - 3s 76ms/step - loss: 1.2648 - accuracy: 0.66 - 3s 74ms/step - loss: 1.2492 - accuracy: 0.66 - 3s 73ms/step - loss: 1.2344 - accuracy: 0.66 - 3s 72ms/step - loss: 1.2209 - accuracy: 0.67 - 3s 70ms/step - loss: 1.2059 - accuracy: 0.67 - 3s 69ms/step - loss: 1.1943 - accuracy: 0.67 - 3s 68ms/step - loss: 1.1874 - accuracy: 0.67 - 3s 66ms/step - loss: 1.1720 - accuracy: 0.68 - 3s 65ms/step - loss: 1.1586 - accuracy: 0.68 - 3s 64ms/step - loss: 1.1448 - accuracy: 0.69 - 3s 63ms/step - loss: 1.1343 - accuracy: 0.69 - 3s 63ms/step - loss: 1.1212 - accuracy: 0.69 - 3s 62ms/step - loss: 1.1110 - accuracy: 0.70 - 3s 61ms/step - loss: 1.0985 - accuracy: 0.70 - 3s 60ms/step - loss: 1.0880 - accuracy: 0.70 - 3s 59ms/step - loss: 1.0796 - accuracy: 0.71 - 3s 58ms/step - loss: 1.0690 - accuracy: 0.71 - 3s 57ms/step - loss: 1.0582 - accuracy: 0.71 - 3s 57ms/step - loss: 1.0508 - accuracy: 0.71 - 3s 56ms/step - loss: 1.0426 - accuracy: 0.71 - 3s 55ms/step - loss: 1.0340 - accuracy: 0.72 - 3s 55ms/step - loss: 1.0239 - accuracy: 0.72 - 3s 54ms/step - loss: 1.0173 - accuracy: 0.72 - 3s 53ms/step - loss: 1.0077 - accuracy: 0.72 - 3s 53ms/step - loss: 0.9969 - accuracy: 0.73 - 3s 52ms/step - loss: 0.9916 - accuracy: 0.73 - 3s 52ms/step - loss: 0.9847 - accuracy: 0.73 - 3s 51ms/step - loss: 0.9775 - accuracy: 0.73 - 3s 51ms/step - loss: 0.9700 - accuracy: 0.73 - 3s 50ms/step - loss: 0.9606 - accuracy: 0.74 - 3s 50ms/step - loss: 0.9549 - accuracy: 0.74 - 3s 49ms/step - loss: 0.9494 - accuracy: 0.74 - 4s 49ms/step - loss: 0.9430 - accuracy: 0.74 - 4s 48ms/step - loss: 0.9376 - accuracy: 0.74 - 4s 48ms/step - loss: 0.9302 - accuracy: 0.74 - 4s 47ms/step - loss: 0.9221 - accuracy: 0.74 - 4s 47ms/step - loss: 0.9137 - accuracy: 0.75 - 4s 47ms/step - loss: 0.9080 - accuracy: 0.75 - 4s 46ms/step - loss: 0.9010 - accuracy: 0.75 - 4s 46ms/step - loss: 0.8942 - accuracy: 0.75 - 4s 45ms/step - loss: 0.8875 - accuracy: 0.75 - 4s 45ms/step - loss: 0.8812 - accuracy: 0.76 - 4s 45ms/step - loss: 0.8771 - accuracy: 0.76 - 4s 44ms/step - loss: 0.8717 - accuracy: 0.76 - 4s 44ms/step - loss: 0.8665 - accuracy: 0.76 - 4s 44ms/step - loss: 0.8616 - accuracy: 0.76 - 4s 43ms/step - loss: 0.8580 - accuracy: 0.76 - 4s 43ms/step - loss: 0.8527 - accuracy: 0.76 - 4s 43ms/step - loss: 0.8479 - accuracy: 0.76 - 4s 43ms/step - loss: 0.8461 - accuracy: 0.76 - 4s 42ms/step - loss: 0.8412 - accuracy: 0.77 - 4s 42ms/step - loss: 0.8376 - accuracy: 0.77 - 4s 42ms/step - loss: 0.8325 - accuracy: 0.77 - 4s 41ms/step - loss: 0.8274 - accuracy: 0.77 - 4s 41ms/step - loss: 0.8248 - accuracy: 0.77 - 4s 41ms/step - loss: 0.8190 - accuracy: 0.77 - 4s 40ms/step - loss: 0.8184 - accuracy: 0.77 - 4s 40ms/step - loss: 0.8135 - accuracy: 0.77 - 4s 40ms/step - loss: 0.8112 - accuracy: 0.77 - 4s 40ms/step - loss: 0.8088 - accuracy: 0.77 - 4s 40ms/step - loss: 0.8045 - accuracy: 0.77 - 4s 39ms/step - loss: 0.8008 - accuracy: 0.78 - 4s 39ms/step - loss: 0.7959 - accuracy: 0.78 - 4s 39ms/step - loss: 0.7917 - accuracy: 0.78 - 4s 38ms/step - loss: 0.7876 - accuracy: 0.78 - 4s 38ms/step - loss: 0.7851 - accuracy: 0.78 - 4s 38ms/step - loss: 0.7799 - accuracy: 0.78 - 4s 38ms/step - loss: 0.7759 - accuracy: 0.78 - 4s 38ms/step - loss: 0.7717 - accuracy: 0.78 - 4s 37ms/step - loss: 0.7680 - accuracy: 0.78 - 4s 37ms/step - loss: 0.7639 - accuracy: 0.79 - 4s 37ms/step - loss: 0.7615 - accuracy: 0.79 - 4s 37ms/step - loss: 0.7577 - accuracy: 0.79 - 4s 37ms/step - loss: 0.7536 - accuracy: 0.79 - 4s 36ms/step - loss: 0.7503 - accuracy: 0.79 - 4s 36ms/step - loss: 0.7460 - accuracy: 0.79 - 4s 36ms/step - loss: 0.7426 - accuracy: 0.79 - 4s 36ms/step - loss: 0.7391 - accuracy: 0.79 - 4s 36ms/step - loss: 0.7347 - accuracy: 0.79 - 4s 35ms/step - loss: 0.7305 - accuracy: 0.79 - 4s 35ms/step - loss: 0.7275 - accuracy: 0.80 - 4s 35ms/step - loss: 0.7233 - accuracy: 0.80 - 4s 35ms/step - loss: 0.7203 - accuracy: 0.80 - 4s 34ms/step - loss: 0.7187 - accuracy: 0.80 - 4s 34ms/step - loss: 0.7154 - accuracy: 0.80 - 4s 34ms/step - loss: 0.7132 - accuracy: 0.80 - 4s 34ms/step - loss: 0.7089 - accuracy: 0.80 - 4s 34ms/step - loss: 0.7060 - accuracy: 0.80 - 4s 34ms/step - loss: 0.7037 - accuracy: 0.80 - 4s 33ms/step - loss: 0.7009 - accuracy: 0.80 - 4s 33ms/step - loss: 0.6964 - accuracy: 0.80 - 4s 33ms/step - loss: 0.6928 - accuracy: 0.80 - 4s 33ms/step - loss: 0.6908 - accuracy: 0.80 - 4s 33ms/step - loss: 0.6872 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6843 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6814 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6785 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6758 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6731 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6706 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6671 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6671 - accuracy: 0.81 - 4s 32ms/step - loss: 0.6643 - accuracy: 0.81 - 4s 31ms/step - loss: 0.6624 - accuracy: 0.81 - 4s 31ms/step - loss: 0.6592 - accuracy: 0.81 - 5s 31ms/step - loss: 0.6578 - accuracy: 0.81 - 5s 31ms/step - loss: 0.6562 - accuracy: 0.81 - 5s 31ms/step - loss: 0.6530 - accuracy: 0.81 - 5s 31ms/step - loss: 0.6507 - accuracy: 0.82 - 5s 31ms/step - loss: 0.6494 - accuracy: 0.82 - 5s 31ms/step - loss: 0.6475 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6459 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6435 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6409 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6376 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6354 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6336 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6315 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6291 - accuracy: 0.82 - 5s 29ms/step - loss: 0.6272 - accuracy: 0.82 - 5s 29ms/step - loss: 0.6244 - accuracy: 0.82 - 5s 29ms/step - loss: 0.6219 - accuracy: 0.82 - 5s 29ms/step - loss: 0.6198 - accuracy: 0.82 - 5s 29ms/step - loss: 0.6184 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6158 - accuracy: 0.82 - 5s 30ms/step - loss: 0.6136 - accuracy: 0.83 - 5s 30ms/step - loss: 0.6128 - accuracy: 0.83 - 5s 29ms/step - loss: 0.6105 - accuracy: 0.83 - 5s 29ms/step - loss: 0.6083 - accuracy: 0.83 - 5s 29ms/step - loss: 0.6059 - accuracy: 0.83 - 5s 29ms/step - loss: 0.6046 - accuracy: 0.83 - 5s 29ms/step - loss: 0.6026 - accuracy: 0.83 - 5s 29ms/step - loss: 0.6002 - accuracy: 0.83 - 5s 29ms/step - loss: 0.5986 - accuracy: 0.8341\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b    347/Unknown - 5s 29ms/step - loss: 0.5964 - accuracy: 0.83 - 5s 29ms/step - loss: 0.5948 - accuracy: 0.83 - 5s 29ms/step - loss: 0.5932 - accuracy: 0.83 - 5s 29ms/step - loss: 0.5907 - accuracy: 0.83 - 5s 29ms/step - loss: 0.5891 - accuracy: 0.83 - 5s 28ms/step - loss: 0.5886 - accuracy: 0.83 - 5s 28ms/step - loss: 0.5865 - accuracy: 0.83 - 5s 28ms/step - loss: 0.5839 - accuracy: 0.83 - 5s 28ms/step - loss: 0.5823 - accuracy: 0.83 - 5s 28ms/step - loss: 0.5805 - accuracy: 0.83 - 5s 28ms/step - loss: 0.5780 - accuracy: 0.84 - 5s 28ms/step - loss: 0.5763 - accuracy: 0.84 - 5s 28ms/step - loss: 0.5744 - accuracy: 0.84 - 5s 28ms/step - loss: 0.5729 - accuracy: 0.84 - 5s 28ms/step - loss: 0.5703 - accuracy: 0.84 - 5s 28ms/step - loss: 0.5680 - accuracy: 0.84 - 5s 28ms/step - loss: 0.5675 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5667 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5651 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5639 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5630 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5614 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5597 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5588 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5568 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5551 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5533 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5520 - accuracy: 0.84 - 5s 27ms/step - loss: 0.5508 - accuracy: 0.84 - 5s 26ms/step - loss: 0.5499 - accuracy: 0.84 - 5s 26ms/step - loss: 0.5489 - accuracy: 0.84 - 5s 26ms/step - loss: 0.5471 - accuracy: 0.84 - 5s 26ms/step - loss: 0.5452 - accuracy: 0.84 - 5s 26ms/step - loss: 0.5444 - accuracy: 0.84 - 5s 26ms/step - loss: 0.5432 - accuracy: 0.85 - 5s 26ms/step - loss: 0.5417 - accuracy: 0.85 - 5s 26ms/step - loss: 0.5401 - accuracy: 0.85 - 5s 26ms/step - loss: 0.5386 - accuracy: 0.85 - 5s 26ms/step - loss: 0.5368 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5358 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5341 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5328 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5316 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5313 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5301 - accuracy: 0.85 - 6s 26ms/step - loss: 0.5288 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5270 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5257 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5257 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5249 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5238 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5224 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5217 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5210 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5198 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5184 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5178 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5164 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5149 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5140 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5131 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5115 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5110 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5102 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5087 - accuracy: 0.85 - 6s 25ms/step - loss: 0.5080 - accuracy: 0.85 - 6s 24ms/step - loss: 0.5070 - accuracy: 0.85 - 6s 24ms/step - loss: 0.5061 - accuracy: 0.85 - 6s 24ms/step - loss: 0.5057 - accuracy: 0.85 - 6s 24ms/step - loss: 0.5052 - accuracy: 0.85 - 6s 24ms/step - loss: 0.5042 - accuracy: 0.85 - 6s 24ms/step - loss: 0.5031 - accuracy: 0.86 - 6s 24ms/step - loss: 0.5021 - accuracy: 0.86 - 6s 24ms/step - loss: 0.5016 - accuracy: 0.86 - 6s 24ms/step - loss: 0.5013 - accuracy: 0.86 - 6s 24ms/step - loss: 0.5005 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4995 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4986 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4977 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4965 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4952 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4943 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4937 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4924 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4918 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4912 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4902 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4896 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4888 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4879 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4867 - accuracy: 0.86 - 6s 24ms/step - loss: 0.4864 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4863 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4854 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4840 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4828 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4824 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4818 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4806 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4796 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4792 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4780 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4772 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4765 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4754 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4748 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4738 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4728 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4719 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4712 - accuracy: 0.86 - 6s 23ms/step - loss: 0.4708 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4700 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4700 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4689 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4682 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4679 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4680 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4673 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4666 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4658 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4654 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4649 - accuracy: 0.86 - 7s 23ms/step - loss: 0.4642 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4633 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4631 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4629 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4629 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4625 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4615 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4610 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4601 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4594 - accuracy: 0.87 - 7s 23ms/step - loss: 0.4584 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4582 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4575 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4566 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4556 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4548 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4540 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4538 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4529 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4524 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4517 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4513 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4504 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4498 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4490 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4483 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4478 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4469 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4462 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4457 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4451 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4447 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4440 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4436 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4428 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4426 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4421 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4419 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4412 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4405 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4398 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4395 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4388 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4382 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4373 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4367 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4358 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4352 - accuracy: 0.87 - 7s 22ms/step - loss: 0.4346 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4343 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4340 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4331 - accuracy: 0.8778    520/Unknown - 8s 22ms/step - loss: 0.4324 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4322 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4314 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4308 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4305 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4303 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4298 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4291 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4284 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4279 - accuracy: 0.87 - 8s 22ms/step - loss: 0.4277 - accuracy: 0.87 - 8s 21ms/step - loss: 0.4271 - accuracy: 0.87 - 8s 21ms/step - loss: 0.4266 - accuracy: 0.87 - 8s 21ms/step - loss: 0.4260 - accuracy: 0.87 - 8s 21ms/step - loss: 0.4254 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4246 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4239 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4234 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4232 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4229 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4225 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4222 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4217 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4210 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4204 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4207 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4204 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4196 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4194 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4189 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4182 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4180 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4174 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4165 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4159 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4151 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4147 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4143 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4141 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4135 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4130 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4126 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4123 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4119 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4112 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4104 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4105 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4098 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4096 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4095 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4090 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4085 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4079 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4074 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4074 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4067 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4059 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4055 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4051 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4048 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4042 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4036 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4030 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4024 - accuracy: 0.88 - 8s 21ms/step - loss: 0.4020 - accuracy: 0.88 - 9s 21ms/step - loss: 0.4014 - accuracy: 0.88 - 9s 21ms/step - loss: 0.4010 - accuracy: 0.88 - 9s 21ms/step - loss: 0.4005 - accuracy: 0.88 - 9s 21ms/step - loss: 0.4001 - accuracy: 0.88 - 9s 21ms/step - loss: 0.3997 - accuracy: 0.88 - 9s 21ms/step - loss: 0.3994 - accuracy: 0.88 - 9s 21ms/step - loss: 0.3988 - accuracy: 0.88 - 9s 21ms/step - loss: 0.3984 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3980 - accuracy: 0.88 - 9s 21ms/step - loss: 0.3977 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3970 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3966 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3964 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3960 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3953 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3954 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3947 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3941 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3936 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3931 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3925 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3921 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3917 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3911 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3909 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3903 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3896 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3895 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3892 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3888 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3884 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3880 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3877 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3873 - accuracy: 0.88 - 9s 20ms/step - loss: 0.3869 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3864 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3858 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3853 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3850 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3844 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3839 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3834 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3829 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3828 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3824 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3818 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3814 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3812 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3808 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3803 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3800 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3796 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3789 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3783 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3780 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3775 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3769 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3763 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3759 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3756 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3751 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3748 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3743 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3740 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3735 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3729 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3725 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3719 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3714 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3711 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3705 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3701 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3697 - accuracy: 0.89 - 9s 20ms/step - loss: 0.3692 - accuracy: 0.89 - 10s 20ms/step - loss: 0.3687 - accuracy: 0.895 - 10s 20ms/step - loss: 0.3682 - accuracy: 0.895 - 10s 20ms/step - loss: 0.3679 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3674 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3673 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3676 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3671 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3667 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3665 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3659 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3656 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3652 - accuracy: 0.895 - 10s 19ms/step - loss: 0.3648 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3644 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3643 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3639 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3636 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3632 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3627 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3621 - accuracy: 0.896 - 10s 19ms/step - loss: 0.3617 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3615 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3610 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3608 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3604 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3598 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3593 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3589 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3588 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3586 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3586 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3586 - accuracy: 0.897 - 10s 19ms/step - loss: 0.3581 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3582 - accuracy: 0.8979    687/Unknown - 10s 19ms/step - loss: 0.3580 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3577 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3574 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3570 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3567 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3562 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3558 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3554 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3553 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3550 - accuracy: 0.898 - 10s 19ms/step - loss: 0.3546 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3543 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3540 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3539 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3537 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3532 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3529 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3525 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3521 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3517 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3516 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3513 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3511 - accuracy: 0.899 - 10s 19ms/step - loss: 0.3507 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3504 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3501 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3496 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3493 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3489 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3486 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3484 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3486 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3480 - accuracy: 0.900 - 10s 19ms/step - loss: 0.3478 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3473 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3471 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3469 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3464 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3460 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3456 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3452 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3452 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3447 - accuracy: 0.901 - 10s 19ms/step - loss: 0.3445 - accuracy: 0.902 - 10s 19ms/step - loss: 0.3444 - accuracy: 0.902 - 10s 19ms/step - loss: 0.3439 - accuracy: 0.902 - 11s 19ms/step - loss: 0.3438 - accuracy: 0.902 - 11s 19ms/step - loss: 0.3434 - accuracy: 0.902 - 11s 19ms/step - loss: 0.3431 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3429 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3425 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3426 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3422 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3417 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3414 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3412 - accuracy: 0.902 - 11s 18ms/step - loss: 0.3408 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3406 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3405 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3404 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3400 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3397 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3394 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3390 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3386 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3383 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3383 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3378 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3377 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3375 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3374 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3370 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3369 - accuracy: 0.903 - 11s 18ms/step - loss: 0.3366 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3365 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3364 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3361 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3360 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3356 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3353 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3350 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3346 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3345 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3344 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3342 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3338 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3338 - accuracy: 0.904 - 11s 18ms/step - loss: 0.3335 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3332 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3328 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3327 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3323 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3323 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3320 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3317 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3317 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3314 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3312 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3310 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3307 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3305 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3305 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3303 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3299 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3297 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3295 - accuracy: 0.905 - 11s 18ms/step - loss: 0.3293 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3292 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3288 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3284 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3280 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3277 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3275 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3272 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3269 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3267 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3263 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3266 - accuracy: 0.906 - 11s 18ms/step - loss: 0.3264 - accuracy: 0.907 - 11s 18ms/step - loss: 0.3263 - accuracy: 0.907 - 11s 18ms/step - loss: 0.3261 - accuracy: 0.907 - 11s 18ms/step - loss: 0.3258 - accuracy: 0.907 - 11s 18ms/step - loss: 0.3256 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3255 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3253 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3251 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3247 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3245 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3248 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3244 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3242 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3240 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3237 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3235 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3237 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3235 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3233 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3231 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3228 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3226 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3225 - accuracy: 0.907 - 12s 18ms/step - loss: 0.3222 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3221 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3219 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3217 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3215 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3215 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3215 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3212 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3209 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3207 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3204 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3200 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3197 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3194 - accuracy: 0.908 - 12s 18ms/step - loss: 0.3190 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3188 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3188 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3185 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3183 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3180 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3179 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3176 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3173 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3171 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3168 - accuracy: 0.909 - 12s 18ms/step - loss: 0.3166 - accuracy: 0.9096    854/Unknown - 12s 18ms/step - loss: 0.3164 - accuracy: 0.909 - 12s 17ms/step - loss: 0.3161 - accuracy: 0.909 - 12s 17ms/step - loss: 0.3158 - accuracy: 0.909 - 12s 17ms/step - loss: 0.3155 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3151 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3147 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3144 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3140 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3139 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3137 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3134 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3131 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3131 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3128 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3127 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3126 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3124 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3123 - accuracy: 0.910 - 12s 17ms/step - loss: 0.3119 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3119 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3118 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3114 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3111 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3109 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3108 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3107 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3104 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3101 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3098 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3095 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3093 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3091 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3089 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3089 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3086 - accuracy: 0.911 - 12s 17ms/step - loss: 0.3087 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3084 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3083 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3082 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3079 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3078 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3077 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3076 - accuracy: 0.912 - 12s 17ms/step - loss: 0.3072 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3072 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3072 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3068 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3067 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3066 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3066 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3063 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3060 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3062 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3059 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3058 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3058 - accuracy: 0.912 - 13s 17ms/step - loss: 0.3056 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3054 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3052 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3049 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3048 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3046 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3047 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3045 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3044 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3041 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3040 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3038 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3035 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3032 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3029 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3026 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3024 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3022 - accuracy: 0.913 - 13s 17ms/step - loss: 0.3020 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3018 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3017 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3015 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3012 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3011 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3009 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3007 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3009 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3007 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3004 - accuracy: 0.914 - 13s 17ms/step - loss: 0.3002 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2999 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2998 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2995 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2995 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2993 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2991 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2989 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2988 - accuracy: 0.914 - 13s 17ms/step - loss: 0.2985 - accuracy: 0.915 - 13s 17ms/step - loss: 0.2984 - accuracy: 0.915 - 13s 17ms/step - loss: 0.2982 - accuracy: 0.915 - 13s 17ms/step - loss: 0.2981 - accuracy: 0.915 - 13s 17ms/step - loss: 0.2979 - accuracy: 0.915 - 13s 17ms/step - loss: 0.2978 - accuracy: 0.915 - 13s 17ms/step - loss: 0.2975 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2973 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2970 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2967 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2965 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2962 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2960 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2957 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2957 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2955 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2952 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2951 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2950 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2949 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2946 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2945 - accuracy: 0.915 - 13s 16ms/step - loss: 0.2945 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2942 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2940 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2937 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2935 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2933 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2933 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2930 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2929 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2928 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2926 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2924 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2921 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2918 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2917 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2915 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2913 - accuracy: 0.916 - 13s 16ms/step - loss: 0.2911 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2909 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2908 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2906 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2905 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2902 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2900 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2898 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2897 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2895 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2894 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2893 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2892 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2890 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2889 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2887 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2885 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2882 - accuracy: 0.917 - 13s 16ms/step - loss: 0.2880 - accuracy: 0.917 - 14s 16ms/step - loss: 0.2878 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2877 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2877 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2875 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2873 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2874 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2872 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2872 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2870 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2868 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2867 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2865 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2863 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2863 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2862 - accuracy: 0.9186938/938 [==============================]0.2859 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2857 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2856 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2856 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2857 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2857 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2856 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2854 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2852 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2851 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2849 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2849 - accuracy: 0.918 - 14s 16ms/step - loss: 0.2847 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2845 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2842 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2840 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2839 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2837 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2837 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2835 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2833 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2831 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2829 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2829 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2826 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2824 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2822 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2819 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2817 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2815 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2814 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2815 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2813 - accuracy: 0.919 - 14s 16ms/step - loss: 0.2811 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2808 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2806 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2804 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2801 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2801 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2800 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2798 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2796 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2794 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2793 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2794 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2793 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2791 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2790 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2790 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2790 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2789 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2788 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2786 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2786 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2785 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2784 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2782 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2779 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2779 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2778 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2777 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2775 - accuracy: 0.920 - 14s 16ms/step - loss: 0.2774 - accuracy: 0.921 - 14s 16ms/step - loss: 0.2772 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2771 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2769 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2767 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2766 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2765 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2763 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2762 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2760 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2759 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2756 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2755 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2753 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2753 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2751 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2750 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2748 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2747 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2746 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2745 - accuracy: 0.921 - 14s 15ms/step - loss: 0.2746 - accuracy: 0.921 - 15s 16ms/step - loss: 0.2746 - accuracy: 0.9216\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - ETA: 2:51 - loss: 0.1076 - accuracy: 0.96 - ETA: 13s - loss: 0.1271 - accuracy: 0.9648 - ETA: 8s - loss: 0.1323 - accuracy: 0.962 - ETA: 7s - loss: 0.1296 - accuracy: 0.96 - ETA: 6s - loss: 0.1255 - accuracy: 0.96 - ETA: 5s - loss: 0.1260 - accuracy: 0.96 - ETA: 5s - loss: 0.1289 - accuracy: 0.96 - ETA: 4s - loss: 0.1290 - accuracy: 0.96 - ETA: 4s - loss: 0.1312 - accuracy: 0.95 - ETA: 4s - loss: 0.1302 - accuracy: 0.95 - ETA: 4s - loss: 0.1262 - accuracy: 0.96 - ETA: 3s - loss: 0.1251 - accuracy: 0.96 - ETA: 3s - loss: 0.1246 - accuracy: 0.96 - ETA: 3s - loss: 0.1239 - accuracy: 0.96 - ETA: 3s - loss: 0.1227 - accuracy: 0.96 - ETA: 3s - loss: 0.1193 - accuracy: 0.96 - ETA: 3s - loss: 0.1209 - accuracy: 0.96 - ETA: 3s - loss: 0.1212 - accuracy: 0.96 - ETA: 3s - loss: 0.1231 - accuracy: 0.96 - ETA: 2s - loss: 0.1221 - accuracy: 0.96 - ETA: 2s - loss: 0.1210 - accuracy: 0.96 - ETA: 2s - loss: 0.1220 - accuracy: 0.96 - ETA: 2s - loss: 0.1206 - accuracy: 0.96 - ETA: 2s - loss: 0.1186 - accuracy: 0.96 - ETA: 2s - loss: 0.1188 - accuracy: 0.96 - ETA: 2s - loss: 0.1178 - accuracy: 0.96 - ETA: 2s - loss: 0.1178 - accuracy: 0.96 - ETA: 2s - loss: 0.1181 - accuracy: 0.96 - ETA: 2s - loss: 0.1182 - accuracy: 0.96 - ETA: 2s - loss: 0.1175 - accuracy: 0.96 - ETA: 2s - loss: 0.1171 - accuracy: 0.96 - ETA: 2s - loss: 0.1161 - accuracy: 0.96 - ETA: 2s - loss: 0.1166 - accuracy: 0.96 - ETA: 2s - loss: 0.1171 - accuracy: 0.96 - ETA: 2s - loss: 0.1171 - accuracy: 0.96 - ETA: 1s - loss: 0.1167 - accuracy: 0.96 - ETA: 1s - loss: 0.1170 - accuracy: 0.96 - ETA: 1s - loss: 0.1176 - accuracy: 0.96 - ETA: 1s - loss: 0.1182 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1185 - accuracy: 0.96 - ETA: 1s - loss: 0.1180 - accuracy: 0.96 - ETA: 1s - loss: 0.1172 - accuracy: 0.96 - ETA: 1s - loss: 0.1165 - accuracy: 0.96 - ETA: 1s - loss: 0.1161 - accuracy: 0.96 - ETA: 1s - loss: 0.1159 - accuracy: 0.96 - ETA: 1s - loss: 0.1155 - accuracy: 0.96 - ETA: 1s - loss: 0.1161 - accuracy: 0.96 - ETA: 1s - loss: 0.1157 - accuracy: 0.96 - ETA: 1s - loss: 0.1157 - accuracy: 0.96 - ETA: 1s - loss: 0.1155 - accuracy: 0.96 - ETA: 1s - loss: 0.1158 - accuracy: 0.96 - ETA: 0s - loss: 0.1155 - accuracy: 0.96 - ETA: 0s - loss: 0.1151 - accuracy: 0.96 - ETA: 0s - loss: 0.1147 - accuracy: 0.96 - ETA: 0s - loss: 0.1143 - accuracy: 0.96 - ETA: 0s - loss: 0.1141 - accuracy: 0.96 - ETA: 0s - loss: 0.1140 - accuracy: 0.96 - ETA: 0s - loss: 0.1137 - accuracy: 0.96 - ETA: 0s - loss: 0.1142 - accuracy: 0.96 - ETA: 0s - loss: 0.1139 - accuracy: 0.96 - ETA: 0s - loss: 0.1132 - accuracy: 0.96 - ETA: 0s - loss: 0.1130 - accuracy: 0.96 - ETA: 0s - loss: 0.1127 - accuracy: 0.96 - ETA: 0s - loss: 0.1121 - accuracy: 0.96 - ETA: 0s - loss: 0.1123 - accuracy: 0.96 - ETA: 0s - loss: 0.1119 - accuracy: 0.96 - ETA: 0s - loss: 0.1117 - accuracy: 0.96 - ETA: 0s - loss: 0.1121 - accuracy: 0.96 - ETA: 0s - loss: 0.1119 - accuracy: 0.96 - ETA: 0s - loss: 0.1118 - accuracy: 0.96 - 4s 4ms/step - loss: 0.1116 - accuracy: 0.9666\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - ETA: 3:03 - loss: 0.0788 - accuracy: 0.96 - ETA: 16s - loss: 0.0578 - accuracy: 0.9877 - ETA: 9s - loss: 0.0808 - accuracy: 0.974 - ETA: 7s - loss: 0.0889 - accuracy: 0.97 - ETA: 6s - loss: 0.0921 - accuracy: 0.97 - ETA: 5s - loss: 0.0911 - accuracy: 0.97 - ETA: 5s - loss: 0.0901 - accuracy: 0.97 - ETA: 5s - loss: 0.0890 - accuracy: 0.97 - ETA: 4s - loss: 0.0885 - accuracy: 0.97 - ETA: 4s - loss: 0.0861 - accuracy: 0.97 - ETA: 4s - loss: 0.0837 - accuracy: 0.97 - ETA: 4s - loss: 0.0820 - accuracy: 0.97 - ETA: 3s - loss: 0.0829 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0836 - accuracy: 0.97 - ETA: 3s - loss: 0.0826 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0830 - accuracy: 0.97 - ETA: 3s - loss: 0.0818 - accuracy: 0.97 - ETA: 3s - loss: 0.0824 - accuracy: 0.97 - ETA: 3s - loss: 0.0827 - accuracy: 0.97 - ETA: 2s - loss: 0.0824 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.97 - ETA: 2s - loss: 0.0805 - accuracy: 0.97 - ETA: 2s - loss: 0.0799 - accuracy: 0.97 - ETA: 2s - loss: 0.0803 - accuracy: 0.97 - ETA: 2s - loss: 0.0809 - accuracy: 0.97 - ETA: 2s - loss: 0.0804 - accuracy: 0.97 - ETA: 2s - loss: 0.0812 - accuracy: 0.97 - ETA: 2s - loss: 0.0812 - accuracy: 0.97 - ETA: 2s - loss: 0.0809 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0826 - accuracy: 0.97 - ETA: 2s - loss: 0.0823 - accuracy: 0.97 - ETA: 1s - loss: 0.0816 - accuracy: 0.97 - ETA: 1s - loss: 0.0814 - accuracy: 0.97 - ETA: 1s - loss: 0.0817 - accuracy: 0.97 - ETA: 1s - loss: 0.0817 - accuracy: 0.97 - ETA: 1s - loss: 0.0815 - accuracy: 0.97 - ETA: 1s - loss: 0.0811 - accuracy: 0.97 - ETA: 1s - loss: 0.0809 - accuracy: 0.97 - ETA: 1s - loss: 0.0813 - accuracy: 0.97 - ETA: 1s - loss: 0.0813 - accuracy: 0.97 - ETA: 1s - loss: 0.0812 - accuracy: 0.97 - ETA: 1s - loss: 0.0811 - accuracy: 0.97 - ETA: 1s - loss: 0.0811 - accuracy: 0.97 - ETA: 1s - loss: 0.0814 - accuracy: 0.97 - ETA: 1s - loss: 0.0811 - accuracy: 0.97 - ETA: 1s - loss: 0.0806 - accuracy: 0.97 - ETA: 1s - loss: 0.0802 - accuracy: 0.97 - ETA: 1s - loss: 0.0797 - accuracy: 0.97 - ETA: 0s - loss: 0.0797 - accuracy: 0.97 - ETA: 0s - loss: 0.0793 - accuracy: 0.97 - ETA: 0s - loss: 0.0795 - accuracy: 0.97 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0790 - accuracy: 0.97 - ETA: 0s - loss: 0.0790 - accuracy: 0.97 - ETA: 0s - loss: 0.0790 - accuracy: 0.97 - ETA: 0s - loss: 0.0786 - accuracy: 0.97 - ETA: 0s - loss: 0.0786 - accuracy: 0.97 - ETA: 0s - loss: 0.0787 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0789 - accuracy: 0.97 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0792 - accuracy: 0.97 - ETA: 0s - loss: 0.0790 - accuracy: 0.97 - ETA: 0s - loss: 0.0794 - accuracy: 0.97 - ETA: 0s - loss: 0.0793 - accuracy: 0.97 - 4s 4ms/step - loss: 0.0792 - accuracy: 0.9757\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - ETA: 2:43 - loss: 0.0858 - accuracy: 0.95 - ETA: 13s - loss: 0.0693 - accuracy: 0.9812 - ETA: 8s - loss: 0.0669 - accuracy: 0.981 - ETA: 7s - loss: 0.0665 - accuracy: 0.98 - ETA: 6s - loss: 0.0638 - accuracy: 0.98 - ETA: 5s - loss: 0.0600 - accuracy: 0.98 - ETA: 4s - loss: 0.0567 - accuracy: 0.98 - ETA: 4s - loss: 0.0565 - accuracy: 0.98 - ETA: 4s - loss: 0.0582 - accuracy: 0.98 - ETA: 4s - loss: 0.0583 - accuracy: 0.98 - ETA: 3s - loss: 0.0571 - accuracy: 0.98 - ETA: 3s - loss: 0.0553 - accuracy: 0.98 - ETA: 3s - loss: 0.0557 - accuracy: 0.98 - ETA: 3s - loss: 0.0568 - accuracy: 0.98 - ETA: 3s - loss: 0.0582 - accuracy: 0.98 - ETA: 3s - loss: 0.0605 - accuracy: 0.98 - ETA: 3s - loss: 0.0598 - accuracy: 0.98 - ETA: 3s - loss: 0.0599 - accuracy: 0.98 - ETA: 3s - loss: 0.0592 - accuracy: 0.98 - ETA: 2s - loss: 0.0588 - accuracy: 0.98 - ETA: 2s - loss: 0.0590 - accuracy: 0.98 - ETA: 2s - loss: 0.0591 - accuracy: 0.98 - ETA: 2s - loss: 0.0607 - accuracy: 0.98 - ETA: 2s - loss: 0.0607 - accuracy: 0.98 - ETA: 2s - loss: 0.0607 - accuracy: 0.98 - ETA: 2s - loss: 0.0605 - accuracy: 0.98 - ETA: 2s - loss: 0.0618 - accuracy: 0.98 - ETA: 2s - loss: 0.0618 - accuracy: 0.98 - ETA: 2s - loss: 0.0621 - accuracy: 0.98 - ETA: 2s - loss: 0.0625 - accuracy: 0.98 - ETA: 2s - loss: 0.0622 - accuracy: 0.98 - ETA: 2s - loss: 0.0616 - accuracy: 0.98 - ETA: 2s - loss: 0.0615 - accuracy: 0.98 - ETA: 1s - loss: 0.0615 - accuracy: 0.98 - ETA: 1s - loss: 0.0608 - accuracy: 0.98 - ETA: 1s - loss: 0.0606 - accuracy: 0.98 - ETA: 1s - loss: 0.0605 - accuracy: 0.98 - ETA: 1s - loss: 0.0602 - accuracy: 0.98 - ETA: 1s - loss: 0.0598 - accuracy: 0.98 - ETA: 1s - loss: 0.0597 - accuracy: 0.98 - ETA: 1s - loss: 0.0600 - accuracy: 0.98 - ETA: 1s - loss: 0.0607 - accuracy: 0.98 - ETA: 1s - loss: 0.0605 - accuracy: 0.98 - ETA: 1s - loss: 0.0604 - accuracy: 0.98 - ETA: 1s - loss: 0.0600 - accuracy: 0.98 - ETA: 1s - loss: 0.0599 - accuracy: 0.98 - ETA: 1s - loss: 0.0596 - accuracy: 0.98 - ETA: 1s - loss: 0.0597 - accuracy: 0.98 - ETA: 1s - loss: 0.0596 - accuracy: 0.98 - ETA: 0s - loss: 0.0594 - accuracy: 0.98 - ETA: 0s - loss: 0.0596 - accuracy: 0.98 - ETA: 0s - loss: 0.0595 - accuracy: 0.98 - ETA: 0s - loss: 0.0599 - accuracy: 0.98 - ETA: 0s - loss: 0.0598 - accuracy: 0.98 - ETA: 0s - loss: 0.0598 - accuracy: 0.98 - ETA: 0s - loss: 0.0601 - accuracy: 0.98 - ETA: 0s - loss: 0.0601 - accuracy: 0.98 - ETA: 0s - loss: 0.0599 - accuracy: 0.98 - ETA: 0s - loss: 0.0598 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0596 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0597 - accuracy: 0.98 - ETA: 0s - loss: 0.0596 - accuracy: 0.98 - 4s 4ms/step - loss: 0.0594 - accuracy: 0.9817\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - ETA: 2:34 - loss: 0.0342 - accuracy: 0.98 - ETA: 13s - loss: 0.0367 - accuracy: 0.9896 - ETA: 8s - loss: 0.0495 - accuracy: 0.987 - ETA: 6s - loss: 0.0499 - accuracy: 0.98 - ETA: 5s - loss: 0.0525 - accuracy: 0.98 - ETA: 5s - loss: 0.0534 - accuracy: 0.98 - ETA: 4s - loss: 0.0506 - accuracy: 0.98 - ETA: 4s - loss: 0.0484 - accuracy: 0.98 - ETA: 4s - loss: 0.0475 - accuracy: 0.98 - ETA: 4s - loss: 0.0459 - accuracy: 0.98 - ETA: 3s - loss: 0.0458 - accuracy: 0.98 - ETA: 3s - loss: 0.0462 - accuracy: 0.98 - ETA: 3s - loss: 0.0477 - accuracy: 0.98 - ETA: 3s - loss: 0.0471 - accuracy: 0.98 - ETA: 3s - loss: 0.0472 - accuracy: 0.98 - ETA: 3s - loss: 0.0470 - accuracy: 0.98 - ETA: 3s - loss: 0.0474 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0503 - accuracy: 0.98 - ETA: 2s - loss: 0.0495 - accuracy: 0.98 - ETA: 2s - loss: 0.0494 - accuracy: 0.98 - ETA: 2s - loss: 0.0493 - accuracy: 0.98 - ETA: 2s - loss: 0.0488 - accuracy: 0.98 - ETA: 2s - loss: 0.0500 - accuracy: 0.98 - ETA: 2s - loss: 0.0500 - accuracy: 0.98 - ETA: 2s - loss: 0.0497 - accuracy: 0.98 - ETA: 2s - loss: 0.0505 - accuracy: 0.98 - ETA: 2s - loss: 0.0504 - accuracy: 0.98 - ETA: 2s - loss: 0.0500 - accuracy: 0.98 - ETA: 2s - loss: 0.0499 - accuracy: 0.98 - ETA: 2s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0496 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0498 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0495 - accuracy: 0.98 - ETA: 1s - loss: 0.0494 - accuracy: 0.98 - ETA: 1s - loss: 0.0491 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 1s - loss: 0.0488 - accuracy: 0.98 - ETA: 1s - loss: 0.0490 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0487 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0485 - accuracy: 0.98 - ETA: 0s - loss: 0.0483 - accuracy: 0.98 - ETA: 0s - loss: 0.0483 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0478 - accuracy: 0.98 - ETA: 0s - loss: 0.0478 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0480 - accuracy: 0.98 - ETA: 0s - loss: 0.0479 - accuracy: 0.98 - ETA: 0s - loss: 0.0476 - accuracy: 0.98 - ETA: 0s - loss: 0.0477 - accuracy: 0.98 - 4s 4ms/step - loss: 0.0478 - accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(training_batches, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.fit` method returns a `History` object which contains a record of training accuracy and loss values at successive epochs, as well as validation accuracy and loss values when applicable. We will discuss the history object in a later lesson. \n",
    "\n",
    "With our model trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 242,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(figsize = (6,9), ncols = 2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0])\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set(title = 'Class Probability', ylabel = 'Class', xlabel = 'Probability')\n",
    "ax2.set_xlim(0,1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW!! Now our network is brilliant. It can accurately predict the digits in our images. Let's take a look again at the loss and accuracy values for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.00 - 0s 163us/sample - loss: 0.0398 - accuracy: 0.9844\n",
      "\n",
      "Loss after trainging: 0.03982092812657356\n",
      "Accuracy after training: 0.984375\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = model.evaluate(image_batch, label_batch)\n",
    "    \n",
    "print(f'\\nLoss after trainging: {loss}')\n",
    "print(f'Accuracy after training: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Create a network with 784 input units, a hidden layer with 128 units, then a hidden layer with 64 units, then a hidden layer with 32 units and finally an output layer with 10 units. Use a ReLu activation function for all the hidden layers and a softmax activation function for the output layer. Then compile the model using an `adam` optimizer, a `sparse_categorical_crossentropy` loss function, and the `accuracy` metric. Finally, print the loss and accuracy of your un-trained model for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 2.4052 - accuracy: 0.0000e+ - 0s 2ms/sample - loss: 2.3729 - accuracy: 0.0312\n",
      "\n",
      "Loss before training: 2.373\n",
      "Accuracy before training: 3.125%\n"
     ]
    }
   ],
   "source": [
    "my_model = tf.keras.Sequential([\n",
    "           tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "           tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "           tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "my_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = my_model.evaluate(image_batch, label_batch)\n",
    "    \n",
    "print(f'\\nLoss before training: {loss:.3f}')\n",
    "print(f'Accuracy before training: {accuracy:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Train the model you created above for 5 epochs and then print the loss and accuracy of your trained model for a single batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    174/Unknown - 1s 577ms/step - loss: 2.3436 - accuracy: 0.078 - 1s 293ms/step - loss: 2.3546 - accuracy: 0.085 - 1s 199ms/step - loss: 2.3028 - accuracy: 0.104 - 1s 151ms/step - loss: 2.2725 - accuracy: 0.152 - 1s 123ms/step - loss: 2.2581 - accuracy: 0.165 - 1s 104ms/step - loss: 2.2330 - accuracy: 0.190 - 1s 91ms/step - loss: 2.2089 - accuracy: 0.214 - 1s 81ms/step - loss: 2.1810 - accuracy: 0.23 - 1s 74ms/step - loss: 2.1609 - accuracy: 0.24 - 1s 67ms/step - loss: 2.1437 - accuracy: 0.25 - 1s 62ms/step - loss: 2.1266 - accuracy: 0.25 - 1s 58ms/step - loss: 2.1095 - accuracy: 0.25 - 1s 54ms/step - loss: 2.0860 - accuracy: 0.26 - 1s 51ms/step - loss: 2.0627 - accuracy: 0.27 - 1s 48ms/step - loss: 2.0450 - accuracy: 0.27 - 1s 46ms/step - loss: 2.0241 - accuracy: 0.28 - 1s 44ms/step - loss: 1.9999 - accuracy: 0.29 - 1s 42ms/step - loss: 1.9805 - accuracy: 0.30 - 1s 40ms/step - loss: 1.9653 - accuracy: 0.31 - 1s 39ms/step - loss: 1.9534 - accuracy: 0.32 - 1s 38ms/step - loss: 1.9245 - accuracy: 0.34 - 1s 36ms/step - loss: 1.9063 - accuracy: 0.35 - 1s 35ms/step - loss: 1.8832 - accuracy: 0.36 - 1s 34ms/step - loss: 1.8544 - accuracy: 0.37 - 1s 33ms/step - loss: 1.8282 - accuracy: 0.39 - 1s 32ms/step - loss: 1.8072 - accuracy: 0.40 - 1s 31ms/step - loss: 1.7823 - accuracy: 0.41 - 1s 30ms/step - loss: 1.7629 - accuracy: 0.41 - 1s 30ms/step - loss: 1.7331 - accuracy: 0.43 - 1s 29ms/step - loss: 1.7114 - accuracy: 0.44 - 1s 28ms/step - loss: 1.6925 - accuracy: 0.45 - 1s 28ms/step - loss: 1.6741 - accuracy: 0.46 - 1s 27ms/step - loss: 1.6564 - accuracy: 0.46 - 1s 27ms/step - loss: 1.6356 - accuracy: 0.47 - 1s 26ms/step - loss: 1.6135 - accuracy: 0.48 - 1s 26ms/step - loss: 1.5915 - accuracy: 0.49 - 1s 25ms/step - loss: 1.5748 - accuracy: 0.49 - 1s 25ms/step - loss: 1.5562 - accuracy: 0.50 - 1s 25ms/step - loss: 1.5357 - accuracy: 0.51 - 1s 24ms/step - loss: 1.5155 - accuracy: 0.51 - 1s 24ms/step - loss: 1.4980 - accuracy: 0.52 - 1s 23ms/step - loss: 1.4788 - accuracy: 0.53 - 1s 23ms/step - loss: 1.4600 - accuracy: 0.53 - 1s 23ms/step - loss: 1.4458 - accuracy: 0.54 - 1s 22ms/step - loss: 1.4249 - accuracy: 0.54 - 1s 22ms/step - loss: 1.4084 - accuracy: 0.55 - 1s 22ms/step - loss: 1.3944 - accuracy: 0.56 - 1s 22ms/step - loss: 1.3791 - accuracy: 0.56 - 1s 22ms/step - loss: 1.3642 - accuracy: 0.57 - 1s 21ms/step - loss: 1.3542 - accuracy: 0.57 - 1s 21ms/step - loss: 1.3371 - accuracy: 0.58 - 1s 21ms/step - loss: 1.3207 - accuracy: 0.58 - 1s 21ms/step - loss: 1.3073 - accuracy: 0.59 - 1s 20ms/step - loss: 1.2951 - accuracy: 0.59 - 1s 20ms/step - loss: 1.2795 - accuracy: 0.60 - 1s 20ms/step - loss: 1.2644 - accuracy: 0.60 - 1s 20ms/step - loss: 1.2516 - accuracy: 0.60 - 1s 20ms/step - loss: 1.2378 - accuracy: 0.61 - 1s 19ms/step - loss: 1.2240 - accuracy: 0.61 - 1s 19ms/step - loss: 1.2119 - accuracy: 0.62 - 1s 19ms/step - loss: 1.1989 - accuracy: 0.62 - 1s 19ms/step - loss: 1.1890 - accuracy: 0.63 - 1s 19ms/step - loss: 1.1776 - accuracy: 0.63 - 1s 19ms/step - loss: 1.1681 - accuracy: 0.63 - 1s 18ms/step - loss: 1.1573 - accuracy: 0.64 - 1s 18ms/step - loss: 1.1492 - accuracy: 0.64 - 1s 18ms/step - loss: 1.1384 - accuracy: 0.64 - 1s 18ms/step - loss: 1.1283 - accuracy: 0.64 - 1s 18ms/step - loss: 1.1223 - accuracy: 0.65 - 1s 18ms/step - loss: 1.1140 - accuracy: 0.65 - 1s 18ms/step - loss: 1.1046 - accuracy: 0.65 - 1s 18ms/step - loss: 1.0974 - accuracy: 0.65 - 1s 18ms/step - loss: 1.0886 - accuracy: 0.66 - 1s 17ms/step - loss: 1.0793 - accuracy: 0.66 - 1s 17ms/step - loss: 1.0702 - accuracy: 0.66 - 1s 17ms/step - loss: 1.0629 - accuracy: 0.67 - 1s 17ms/step - loss: 1.0548 - accuracy: 0.67 - 1s 17ms/step - loss: 1.0457 - accuracy: 0.67 - 1s 17ms/step - loss: 1.0381 - accuracy: 0.67 - 1s 17ms/step - loss: 1.0307 - accuracy: 0.68 - 1s 17ms/step - loss: 1.0251 - accuracy: 0.68 - 1s 17ms/step - loss: 1.0173 - accuracy: 0.68 - 1s 17ms/step - loss: 1.0123 - accuracy: 0.68 - 1s 16ms/step - loss: 1.0036 - accuracy: 0.68 - 1s 16ms/step - loss: 0.9984 - accuracy: 0.68 - 1s 16ms/step - loss: 0.9906 - accuracy: 0.69 - 1s 16ms/step - loss: 0.9835 - accuracy: 0.69 - 1s 16ms/step - loss: 0.9775 - accuracy: 0.69 - 1s 16ms/step - loss: 0.9728 - accuracy: 0.69 - 1s 16ms/step - loss: 0.9649 - accuracy: 0.69 - 1s 16ms/step - loss: 0.9590 - accuracy: 0.70 - 1s 16ms/step - loss: 0.9529 - accuracy: 0.70 - 1s 16ms/step - loss: 0.9480 - accuracy: 0.70 - 1s 16ms/step - loss: 0.9427 - accuracy: 0.70 - 1s 16ms/step - loss: 0.9354 - accuracy: 0.70 - 2s 16ms/step - loss: 0.9300 - accuracy: 0.71 - 2s 16ms/step - loss: 0.9253 - accuracy: 0.71 - 2s 16ms/step - loss: 0.9207 - accuracy: 0.71 - 2s 16ms/step - loss: 0.9164 - accuracy: 0.71 - 2s 16ms/step - loss: 0.9107 - accuracy: 0.71 - 2s 16ms/step - loss: 0.9055 - accuracy: 0.71 - 2s 15ms/step - loss: 0.9020 - accuracy: 0.71 - 2s 15ms/step - loss: 0.8969 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8906 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8853 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8799 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8762 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8716 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8668 - accuracy: 0.72 - 2s 15ms/step - loss: 0.8632 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8596 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8578 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8549 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8505 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8473 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8433 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8389 - accuracy: 0.73 - 2s 15ms/step - loss: 0.8360 - accuracy: 0.74 - 2s 15ms/step - loss: 0.8332 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8301 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8270 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8242 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8203 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8157 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8123 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8085 - accuracy: 0.74 - 2s 14ms/step - loss: 0.8050 - accuracy: 0.75 - 2s 14ms/step - loss: 0.8029 - accuracy: 0.75 - 2s 14ms/step - loss: 0.8005 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7970 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7928 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7891 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7852 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7816 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7783 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7753 - accuracy: 0.75 - 2s 14ms/step - loss: 0.7721 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7692 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7656 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7630 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7608 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7580 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7543 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7512 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7482 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7447 - accuracy: 0.76 - 2s 14ms/step - loss: 0.7408 - accuracy: 0.77 - 2s 14ms/step - loss: 0.7376 - accuracy: 0.77 - 2s 14ms/step - loss: 0.7347 - accuracy: 0.77 - 2s 14ms/step - loss: 0.7325 - accuracy: 0.77 - 2s 14ms/step - loss: 0.7304 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7289 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7262 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7241 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7234 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7209 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7174 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7158 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7137 - accuracy: 0.77 - 2s 13ms/step - loss: 0.7126 - accuracy: 0.78 - 2s 13ms/step - loss: 0.7109 - accuracy: 0.78 - 2s 13ms/step - loss: 0.7076 - accuracy: 0.78 - 2s 13ms/step - loss: 0.7056 - accuracy: 0.78 - 2s 13ms/step - loss: 0.7035 - accuracy: 0.78 - 2s 13ms/step - loss: 0.7016 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6990 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6978 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6946 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6931 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6916 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6889 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6865 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6836 - accuracy: 0.78 - 2s 13ms/step - loss: 0.6813 - accuracy: 0.7903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b    348/Unknown - 2s 13ms/step - loss: 0.6794 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6776 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6758 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6737 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6718 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6695 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6673 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6653 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6631 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6608 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6584 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6564 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6542 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6522 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6499 - accuracy: 0.79 - 2s 13ms/step - loss: 0.6481 - accuracy: 0.80 - 2s 13ms/step - loss: 0.6463 - accuracy: 0.80 - 2s 13ms/step - loss: 0.6448 - accuracy: 0.80 - 2s 13ms/step - loss: 0.6434 - accuracy: 0.80 - 2s 13ms/step - loss: 0.6420 - accuracy: 0.80 - 2s 13ms/step - loss: 0.6399 - accuracy: 0.80 - 2s 12ms/step - loss: 0.6378 - accuracy: 0.80 - 2s 12ms/step - loss: 0.6358 - accuracy: 0.80 - 2s 12ms/step - loss: 0.6338 - accuracy: 0.80 - 2s 12ms/step - loss: 0.6323 - accuracy: 0.80 - 2s 12ms/step - loss: 0.6306 - accuracy: 0.80 - 3s 12ms/step - loss: 0.6289 - accuracy: 0.80 - 3s 13ms/step - loss: 0.6271 - accuracy: 0.80 - 3s 13ms/step - loss: 0.6248 - accuracy: 0.80 - 3s 12ms/step - loss: 0.6235 - accuracy: 0.80 - 3s 12ms/step - loss: 0.6210 - accuracy: 0.80 - 3s 12ms/step - loss: 0.6195 - accuracy: 0.80 - 3s 12ms/step - loss: 0.6180 - accuracy: 0.80 - 3s 12ms/step - loss: 0.6157 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6141 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6133 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6121 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6100 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6080 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6072 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6054 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6044 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6037 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6020 - accuracy: 0.81 - 3s 12ms/step - loss: 0.6012 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5999 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5985 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5971 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5960 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5949 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5933 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5916 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5900 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5884 - accuracy: 0.81 - 3s 12ms/step - loss: 0.5867 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5847 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5835 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5825 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5809 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5798 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5784 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5771 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5762 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5755 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5755 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5742 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5734 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5728 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5712 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5693 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5679 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5663 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5655 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5648 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5634 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5621 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5608 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5592 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5588 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5579 - accuracy: 0.82 - 3s 12ms/step - loss: 0.5572 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5557 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5548 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5540 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5526 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5522 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5514 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5500 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5487 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5488 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5479 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5468 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5455 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5442 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5437 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5424 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5420 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5408 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5400 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5396 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5388 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5385 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5376 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5371 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5359 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5350 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5337 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5331 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5319 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5308 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5299 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5288 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5276 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5271 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5260 - accuracy: 0.83 - 3s 12ms/step - loss: 0.5251 - accuracy: 0.84 - 3s 12ms/step - loss: 0.5247 - accuracy: 0.84 - 3s 12ms/step - loss: 0.5241 - accuracy: 0.84 - 3s 12ms/step - loss: 0.5239 - accuracy: 0.84 - 3s 12ms/step - loss: 0.5233 - accuracy: 0.84 - 3s 12ms/step - loss: 0.5221 - accuracy: 0.84 - 3s 12ms/step - loss: 0.5218 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5215 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5206 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5198 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5188 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5176 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5171 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5161 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5153 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5145 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5137 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5128 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5121 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5110 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5107 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5098 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5090 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5082 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5078 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5068 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5060 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5056 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5046 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5035 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5024 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5013 - accuracy: 0.84 - 4s 12ms/step - loss: 0.5004 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4994 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4990 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4979 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4969 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4961 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4950 - accuracy: 0.84 - 4s 12ms/step - loss: 0.4946 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4936 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4932 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4931 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4922 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4915 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4904 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4900 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4893 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4891 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4880 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4877 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4869 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4860 - accuracy: 0.85 - 4s 12ms/step - loss: 0.4858 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4850 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4840 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4830 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4826 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4819 - accuracy: 0.8541    522/Unknown - 4s 11ms/step - loss: 0.4812 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4806 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4802 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4793 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4785 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4777 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4768 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4761 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4753 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4744 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4735 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4728 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4722 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4719 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4716 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4712 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4706 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4697 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4690 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4686 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4679 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4673 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4667 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4659 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4652 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4648 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4643 - accuracy: 0.85 - 4s 11ms/step - loss: 0.4636 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4627 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4621 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4612 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4605 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4596 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4593 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4592 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4585 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4584 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4581 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4573 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4565 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4557 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4549 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4544 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4536 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4528 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4526 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4520 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4515 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4512 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4508 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4500 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4494 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4495 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4487 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4479 - accuracy: 0.86 - 4s 11ms/step - loss: 0.4474 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4469 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4464 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4459 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4451 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4444 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4440 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4432 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4426 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4421 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4412 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4403 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4396 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4389 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4385 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4379 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4374 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4370 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4366 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4363 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4365 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4359 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4355 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4347 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4346 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4342 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4339 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4331 - accuracy: 0.86 - 5s 11ms/step - loss: 0.4324 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4319 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4311 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4304 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4297 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4291 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4286 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4280 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4274 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4268 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4262 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4255 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4250 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4243 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4239 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4233 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4232 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4227 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4223 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4219 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4212 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4204 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4198 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4191 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4189 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4182 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4175 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4168 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4161 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4158 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4150 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4148 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4142 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4136 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4129 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4124 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4118 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4117 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4114 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4111 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4108 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4102 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4103 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4098 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4092 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4087 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4083 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4078 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4074 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4071 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4065 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4061 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4057 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4052 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4047 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4041 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4035 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4030 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4026 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4021 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4018 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4014 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4010 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4004 - accuracy: 0.87 - 5s 11ms/step - loss: 0.4000 - accuracy: 0.87 - 5s 11ms/step - loss: 0.3995 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3994 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3991 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3985 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3980 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3977 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3972 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3967 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3963 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3957 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3953 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3949 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3946 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3942 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3939 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3933 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3930 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3928 - accuracy: 0.88 - 5s 11ms/step - loss: 0.3924 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3923 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3919 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3916 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3919 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3919 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3915 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3909 - accuracy: 0.8827    696/Unknown - 6s 11ms/step - loss: 0.3905 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3903 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3901 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3894 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3889 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3885 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3882 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3879 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3876 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3875 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3873 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3868 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3864 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3860 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3856 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3853 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3849 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3848 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3845 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3845 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3847 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3843 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3842 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3842 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3839 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3833 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3833 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3828 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3823 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3822 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3817 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3816 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3811 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3806 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3802 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3798 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3795 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3792 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3790 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3785 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3781 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3778 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3776 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3773 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3769 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3768 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3766 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3761 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3757 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3754 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3752 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3749 - accuracy: 0.88 - 6s 11ms/step - loss: 0.3746 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3742 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3738 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3733 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3728 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3725 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3723 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3719 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3715 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3711 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3708 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3707 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3701 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3699 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3695 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3694 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3691 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3690 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3689 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3685 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3683 - accuracy: 0.88 - 6s 10ms/step - loss: 0.3679 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3678 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3674 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3671 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3666 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3665 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3663 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3660 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3658 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3654 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3649 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3648 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3644 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3641 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3640 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3636 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3632 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3628 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3626 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3622 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3617 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3614 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3613 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3609 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3607 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3606 - accuracy: 0.89 - 6s 10ms/step - loss: 0.3606 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3603 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3599 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3597 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3593 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3590 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3588 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3585 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3582 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3578 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3574 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3573 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3569 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3565 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3561 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3556 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3552 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3549 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3545 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3544 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3542 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3539 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3535 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3532 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3528 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3525 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3520 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3516 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3513 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3509 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3506 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3504 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3501 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3499 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3499 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3499 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3494 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3492 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3489 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3488 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3487 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3486 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3483 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3481 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3479 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3476 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3471 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3468 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3464 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3460 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3458 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3455 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3452 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3449 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3447 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3443 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3440 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3438 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3435 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3435 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3431 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3429 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3427 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3423 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3420 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3417 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3415 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3412 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3410 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3410 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3407 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3407 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3404 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3403 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3401 - accuracy: 0.8988    870/Unknown - 7s 10ms/step - loss: 0.3399 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3395 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3392 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3390 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3390 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3388 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3387 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3384 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3381 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3378 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3376 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3374 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3374 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3371 - accuracy: 0.89 - 7s 10ms/step - loss: 0.3367 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3365 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3362 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3360 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3356 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3352 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3349 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3347 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3343 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3340 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3337 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3334 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3332 - accuracy: 0.90 - 7s 10ms/step - loss: 0.3330 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3329 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3327 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3324 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3322 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3318 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3316 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3312 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3312 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3309 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3307 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3303 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3302 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3299 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3296 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3293 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3293 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3292 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3289 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3286 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3285 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3282 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3281 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3281 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3278 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3278 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3279 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3276 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3275 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3274 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3273 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3271 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3268 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3266 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3263 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3260 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3258 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3259 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3257 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3255 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3252 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3249 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3246 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3245 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3243 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3242 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3240 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3237 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3237 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3234 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3232 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3229 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3227 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3228 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3225 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3224 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3222 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3220 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3217 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3214 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3211 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3209 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3206 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3203 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3200 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3199 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3198 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3197 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3194 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3192 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3194 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3193 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3190 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3187 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3185 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3183 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3181 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3179 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3176 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3175 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3172 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3170 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3169 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3166 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3168 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3168 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3166 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3163 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3161 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3159 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3156 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3154 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3151 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3149 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3149 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3146 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3145 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3142 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3140 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3138 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3136 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3133 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3131 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3131 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3130 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3129 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3127 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3126 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3124 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3122 - accuracy: 0.90 - 8s 10ms/step - loss: 0.3119 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3119 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3116 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3114 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3111 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3108 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3106 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3104 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3102 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3102 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3099 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3098 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3095 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3094 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3093 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3089 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3086 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3084 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3083 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3080 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3080 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3081 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3081 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3079 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3076 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3073 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3072 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3072 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3071 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3070 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3067 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3065 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3065 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3064 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3061 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3059 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3058 - accuracy: 0.9092938/938 [==============================].3056 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3053 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3051 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3049 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3048 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3047 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3045 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3044 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3042 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3042 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3041 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3039 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3037 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3035 - accuracy: 0.90 - 9s 10ms/step - loss: 0.3034 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3032 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3030 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3028 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3026 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3024 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3024 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3022 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3024 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3022 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3020 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3017 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3015 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3013 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3012 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3010 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3009 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3007 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3007 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3005 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3003 - accuracy: 0.91 - 9s 10ms/step - loss: 0.3001 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2998 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2997 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2995 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2994 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2992 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2989 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2989 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2987 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2985 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2983 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2983 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2981 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2979 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2976 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2976 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2975 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2973 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2972 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2971 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2969 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2966 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2965 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2965 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2964 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2964 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2962 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2960 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2957 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2955 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2953 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2950 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2948 - accuracy: 0.91 - 9s 10ms/step - loss: 0.2948 - accuracy: 0.9126\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - ETA: 2:49 - loss: 0.0791 - accuracy: 0.96 - ETA: 14s - loss: 0.1028 - accuracy: 0.9708 - ETA: 8s - loss: 0.1063 - accuracy: 0.968 - ETA: 7s - loss: 0.1157 - accuracy: 0.96 - ETA: 6s - loss: 0.1240 - accuracy: 0.96 - ETA: 5s - loss: 0.1283 - accuracy: 0.96 - ETA: 5s - loss: 0.1242 - accuracy: 0.96 - ETA: 4s - loss: 0.1296 - accuracy: 0.96 - ETA: 4s - loss: 0.1312 - accuracy: 0.96 - ETA: 4s - loss: 0.1346 - accuracy: 0.95 - ETA: 4s - loss: 0.1335 - accuracy: 0.96 - ETA: 3s - loss: 0.1304 - accuracy: 0.96 - ETA: 3s - loss: 0.1316 - accuracy: 0.96 - ETA: 3s - loss: 0.1324 - accuracy: 0.96 - ETA: 3s - loss: 0.1334 - accuracy: 0.96 - ETA: 3s - loss: 0.1351 - accuracy: 0.96 - ETA: 3s - loss: 0.1351 - accuracy: 0.96 - ETA: 3s - loss: 0.1345 - accuracy: 0.96 - ETA: 3s - loss: 0.1344 - accuracy: 0.96 - ETA: 2s - loss: 0.1322 - accuracy: 0.96 - ETA: 2s - loss: 0.1325 - accuracy: 0.96 - ETA: 2s - loss: 0.1315 - accuracy: 0.96 - ETA: 2s - loss: 0.1319 - accuracy: 0.96 - ETA: 2s - loss: 0.1314 - accuracy: 0.96 - ETA: 2s - loss: 0.1310 - accuracy: 0.96 - ETA: 2s - loss: 0.1313 - accuracy: 0.96 - ETA: 2s - loss: 0.1299 - accuracy: 0.96 - ETA: 2s - loss: 0.1289 - accuracy: 0.96 - ETA: 2s - loss: 0.1278 - accuracy: 0.96 - ETA: 2s - loss: 0.1283 - accuracy: 0.96 - ETA: 2s - loss: 0.1269 - accuracy: 0.96 - ETA: 2s - loss: 0.1259 - accuracy: 0.96 - ETA: 2s - loss: 0.1258 - accuracy: 0.96 - ETA: 1s - loss: 0.1246 - accuracy: 0.96 - ETA: 1s - loss: 0.1250 - accuracy: 0.96 - ETA: 1s - loss: 0.1250 - accuracy: 0.96 - ETA: 1s - loss: 0.1249 - accuracy: 0.96 - ETA: 1s - loss: 0.1251 - accuracy: 0.96 - ETA: 1s - loss: 0.1249 - accuracy: 0.96 - ETA: 1s - loss: 0.1245 - accuracy: 0.96 - ETA: 1s - loss: 0.1240 - accuracy: 0.96 - ETA: 1s - loss: 0.1235 - accuracy: 0.96 - ETA: 1s - loss: 0.1237 - accuracy: 0.96 - ETA: 1s - loss: 0.1237 - accuracy: 0.96 - ETA: 1s - loss: 0.1239 - accuracy: 0.96 - ETA: 1s - loss: 0.1230 - accuracy: 0.96 - ETA: 1s - loss: 0.1233 - accuracy: 0.96 - ETA: 1s - loss: 0.1229 - accuracy: 0.96 - ETA: 1s - loss: 0.1221 - accuracy: 0.96 - ETA: 1s - loss: 0.1209 - accuracy: 0.96 - ETA: 0s - loss: 0.1202 - accuracy: 0.96 - ETA: 0s - loss: 0.1200 - accuracy: 0.96 - ETA: 0s - loss: 0.1192 - accuracy: 0.96 - ETA: 0s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1186 - accuracy: 0.96 - ETA: 0s - loss: 0.1185 - accuracy: 0.96 - ETA: 0s - loss: 0.1183 - accuracy: 0.96 - ETA: 0s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1170 - accuracy: 0.96 - ETA: 0s - loss: 0.1171 - accuracy: 0.96 - ETA: 0s - loss: 0.1173 - accuracy: 0.96 - ETA: 0s - loss: 0.1179 - accuracy: 0.96 - ETA: 0s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1177 - accuracy: 0.96 - ETA: 0s - loss: 0.1174 - accuracy: 0.96 - ETA: 0s - loss: 0.1172 - accuracy: 0.96 - ETA: 0s - loss: 0.1168 - accuracy: 0.96 - ETA: 0s - loss: 0.1169 - accuracy: 0.96 - ETA: 0s - loss: 0.1171 - accuracy: 0.96 - ETA: 0s - loss: 0.1168 - accuracy: 0.96 - 4s 4ms/step - loss: 0.1170 - accuracy: 0.9645\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - ETA: 3:22 - loss: 0.2180 - accuracy: 0.92 - ETA: 19s - loss: 0.1202 - accuracy: 0.9603 - ETA: 12s - loss: 0.1110 - accuracy: 0.963 - ETA: 9s - loss: 0.1051 - accuracy: 0.964 - ETA: 7s - loss: 0.1106 - accuracy: 0.96 - ETA: 6s - loss: 0.1026 - accuracy: 0.96 - ETA: 6s - loss: 0.0994 - accuracy: 0.96 - ETA: 5s - loss: 0.0925 - accuracy: 0.97 - ETA: 5s - loss: 0.0909 - accuracy: 0.97 - ETA: 5s - loss: 0.0915 - accuracy: 0.97 - ETA: 4s - loss: 0.0905 - accuracy: 0.97 - ETA: 4s - loss: 0.0895 - accuracy: 0.97 - ETA: 4s - loss: 0.0899 - accuracy: 0.97 - ETA: 3s - loss: 0.0885 - accuracy: 0.97 - ETA: 3s - loss: 0.0878 - accuracy: 0.97 - ETA: 3s - loss: 0.0885 - accuracy: 0.97 - ETA: 3s - loss: 0.0889 - accuracy: 0.97 - ETA: 3s - loss: 0.0880 - accuracy: 0.97 - ETA: 3s - loss: 0.0863 - accuracy: 0.97 - ETA: 3s - loss: 0.0860 - accuracy: 0.97 - ETA: 3s - loss: 0.0856 - accuracy: 0.97 - ETA: 3s - loss: 0.0848 - accuracy: 0.97 - ETA: 3s - loss: 0.0839 - accuracy: 0.97 - ETA: 2s - loss: 0.0832 - accuracy: 0.97 - ETA: 2s - loss: 0.0843 - accuracy: 0.97 - ETA: 2s - loss: 0.0842 - accuracy: 0.97 - ETA: 2s - loss: 0.0836 - accuracy: 0.97 - ETA: 2s - loss: 0.0826 - accuracy: 0.97 - ETA: 2s - loss: 0.0824 - accuracy: 0.97 - ETA: 2s - loss: 0.0827 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0830 - accuracy: 0.97 - ETA: 2s - loss: 0.0831 - accuracy: 0.97 - ETA: 2s - loss: 0.0834 - accuracy: 0.97 - ETA: 2s - loss: 0.0831 - accuracy: 0.97 - ETA: 2s - loss: 0.0830 - accuracy: 0.97 - ETA: 2s - loss: 0.0824 - accuracy: 0.97 - ETA: 1s - loss: 0.0826 - accuracy: 0.97 - ETA: 1s - loss: 0.0823 - accuracy: 0.97 - ETA: 1s - loss: 0.0822 - accuracy: 0.97 - ETA: 1s - loss: 0.0832 - accuracy: 0.97 - ETA: 1s - loss: 0.0828 - accuracy: 0.97 - ETA: 1s - loss: 0.0831 - accuracy: 0.97 - ETA: 1s - loss: 0.0831 - accuracy: 0.97 - ETA: 1s - loss: 0.0833 - accuracy: 0.97 - ETA: 1s - loss: 0.0824 - accuracy: 0.97 - ETA: 1s - loss: 0.0819 - accuracy: 0.97 - ETA: 1s - loss: 0.0820 - accuracy: 0.97 - ETA: 1s - loss: 0.0822 - accuracy: 0.97 - ETA: 1s - loss: 0.0822 - accuracy: 0.97 - ETA: 1s - loss: 0.0825 - accuracy: 0.97 - ETA: 1s - loss: 0.0827 - accuracy: 0.97 - ETA: 1s - loss: 0.0821 - accuracy: 0.97 - ETA: 1s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0819 - accuracy: 0.97 - ETA: 0s - loss: 0.0824 - accuracy: 0.97 - ETA: 0s - loss: 0.0828 - accuracy: 0.97 - ETA: 0s - loss: 0.0823 - accuracy: 0.97 - ETA: 0s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0821 - accuracy: 0.97 - ETA: 0s - loss: 0.0825 - accuracy: 0.97 - ETA: 0s - loss: 0.0825 - accuracy: 0.97 - ETA: 0s - loss: 0.0822 - accuracy: 0.97 - ETA: 0s - loss: 0.0820 - accuracy: 0.97 - ETA: 0s - loss: 0.0817 - accuracy: 0.97 - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - ETA: 0s - loss: 0.0815 - accuracy: 0.97 - ETA: 0s - loss: 0.0812 - accuracy: 0.97 - ETA: 0s - loss: 0.0811 - accuracy: 0.97 - ETA: 0s - loss: 0.0811 - accuracy: 0.97 - ETA: 0s - loss: 0.0809 - accuracy: 0.97 - 4s 4ms/step - loss: 0.0808 - accuracy: 0.9750\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - ETA: 2:56 - loss: 0.0868 - accuracy: 0.96 - ETA: 15s - loss: 0.0766 - accuracy: 0.9743 - ETA: 9s - loss: 0.0702 - accuracy: 0.978 - ETA: 7s - loss: 0.0645 - accuracy: 0.97 - ETA: 6s - loss: 0.0598 - accuracy: 0.98 - ETA: 5s - loss: 0.0593 - accuracy: 0.98 - ETA: 5s - loss: 0.0586 - accuracy: 0.98 - ETA: 4s - loss: 0.0603 - accuracy: 0.98 - ETA: 4s - loss: 0.0651 - accuracy: 0.98 - ETA: 4s - loss: 0.0659 - accuracy: 0.98 - ETA: 4s - loss: 0.0654 - accuracy: 0.98 - ETA: 3s - loss: 0.0646 - accuracy: 0.98 - ETA: 3s - loss: 0.0649 - accuracy: 0.98 - ETA: 3s - loss: 0.0647 - accuracy: 0.98 - ETA: 3s - loss: 0.0632 - accuracy: 0.98 - ETA: 3s - loss: 0.0633 - accuracy: 0.98 - ETA: 3s - loss: 0.0632 - accuracy: 0.98 - ETA: 3s - loss: 0.0624 - accuracy: 0.98 - ETA: 3s - loss: 0.0612 - accuracy: 0.98 - ETA: 2s - loss: 0.0603 - accuracy: 0.98 - ETA: 2s - loss: 0.0602 - accuracy: 0.98 - ETA: 2s - loss: 0.0600 - accuracy: 0.98 - ETA: 2s - loss: 0.0594 - accuracy: 0.98 - ETA: 2s - loss: 0.0602 - accuracy: 0.98 - ETA: 2s - loss: 0.0614 - accuracy: 0.98 - ETA: 2s - loss: 0.0618 - accuracy: 0.98 - ETA: 2s - loss: 0.0616 - accuracy: 0.98 - ETA: 2s - loss: 0.0610 - accuracy: 0.98 - ETA: 2s - loss: 0.0617 - accuracy: 0.98 - ETA: 2s - loss: 0.0619 - accuracy: 0.98 - ETA: 2s - loss: 0.0621 - accuracy: 0.98 - ETA: 2s - loss: 0.0624 - accuracy: 0.98 - ETA: 2s - loss: 0.0627 - accuracy: 0.98 - ETA: 1s - loss: 0.0626 - accuracy: 0.98 - ETA: 1s - loss: 0.0627 - accuracy: 0.98 - ETA: 1s - loss: 0.0628 - accuracy: 0.98 - ETA: 1s - loss: 0.0626 - accuracy: 0.98 - ETA: 1s - loss: 0.0628 - accuracy: 0.97 - ETA: 1s - loss: 0.0630 - accuracy: 0.97 - ETA: 1s - loss: 0.0629 - accuracy: 0.97 - ETA: 1s - loss: 0.0629 - accuracy: 0.98 - ETA: 1s - loss: 0.0631 - accuracy: 0.98 - ETA: 1s - loss: 0.0629 - accuracy: 0.98 - ETA: 1s - loss: 0.0628 - accuracy: 0.98 - ETA: 1s - loss: 0.0632 - accuracy: 0.97 - ETA: 1s - loss: 0.0629 - accuracy: 0.97 - ETA: 1s - loss: 0.0631 - accuracy: 0.97 - ETA: 1s - loss: 0.0632 - accuracy: 0.98 - ETA: 1s - loss: 0.0631 - accuracy: 0.98 - ETA: 1s - loss: 0.0629 - accuracy: 0.98 - ETA: 0s - loss: 0.0626 - accuracy: 0.98 - ETA: 0s - loss: 0.0624 - accuracy: 0.98 - ETA: 0s - loss: 0.0620 - accuracy: 0.98 - ETA: 0s - loss: 0.0620 - accuracy: 0.98 - ETA: 0s - loss: 0.0617 - accuracy: 0.98 - ETA: 0s - loss: 0.0616 - accuracy: 0.98 - ETA: 0s - loss: 0.0615 - accuracy: 0.98 - ETA: 0s - loss: 0.0612 - accuracy: 0.98 - ETA: 0s - loss: 0.0615 - accuracy: 0.98 - ETA: 0s - loss: 0.0615 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.98 - ETA: 0s - loss: 0.0611 - accuracy: 0.98 - ETA: 0s - loss: 0.0614 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.98 - ETA: 0s - loss: 0.0613 - accuracy: 0.98 - ETA: 0s - loss: 0.0614 - accuracy: 0.98 - 4s 4ms/step - loss: 0.0616 - accuracy: 0.9804\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - ETA: 2:00 - loss: 0.0366 - accuracy: 0.98 - ETA: 9s - loss: 0.0506 - accuracy: 0.9825 - ETA: 6s - loss: 0.0465 - accuracy: 0.98 - ETA: 5s - loss: 0.0463 - accuracy: 0.98 - ETA: 4s - loss: 0.0432 - accuracy: 0.98 - ETA: 4s - loss: 0.0461 - accuracy: 0.98 - ETA: 4s - loss: 0.0494 - accuracy: 0.98 - ETA: 3s - loss: 0.0496 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0488 - accuracy: 0.98 - ETA: 3s - loss: 0.0484 - accuracy: 0.98 - ETA: 3s - loss: 0.0491 - accuracy: 0.98 - ETA: 3s - loss: 0.0495 - accuracy: 0.98 - ETA: 2s - loss: 0.0495 - accuracy: 0.98 - ETA: 2s - loss: 0.0526 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0526 - accuracy: 0.98 - ETA: 2s - loss: 0.0536 - accuracy: 0.98 - ETA: 2s - loss: 0.0537 - accuracy: 0.98 - ETA: 2s - loss: 0.0535 - accuracy: 0.98 - ETA: 2s - loss: 0.0533 - accuracy: 0.98 - ETA: 2s - loss: 0.0533 - accuracy: 0.98 - ETA: 2s - loss: 0.0536 - accuracy: 0.98 - ETA: 2s - loss: 0.0535 - accuracy: 0.98 - ETA: 2s - loss: 0.0530 - accuracy: 0.98 - ETA: 1s - loss: 0.0526 - accuracy: 0.98 - ETA: 1s - loss: 0.0524 - accuracy: 0.98 - ETA: 1s - loss: 0.0522 - accuracy: 0.98 - ETA: 1s - loss: 0.0524 - accuracy: 0.98 - ETA: 1s - loss: 0.0521 - accuracy: 0.98 - ETA: 1s - loss: 0.0517 - accuracy: 0.98 - ETA: 1s - loss: 0.0521 - accuracy: 0.98 - ETA: 1s - loss: 0.0523 - accuracy: 0.98 - ETA: 1s - loss: 0.0525 - accuracy: 0.98 - ETA: 1s - loss: 0.0523 - accuracy: 0.98 - ETA: 1s - loss: 0.0528 - accuracy: 0.98 - ETA: 1s - loss: 0.0525 - accuracy: 0.98 - ETA: 1s - loss: 0.0522 - accuracy: 0.98 - ETA: 1s - loss: 0.0524 - accuracy: 0.98 - ETA: 1s - loss: 0.0522 - accuracy: 0.98 - ETA: 1s - loss: 0.0520 - accuracy: 0.98 - ETA: 1s - loss: 0.0518 - accuracy: 0.98 - ETA: 1s - loss: 0.0520 - accuracy: 0.98 - ETA: 0s - loss: 0.0518 - accuracy: 0.98 - ETA: 0s - loss: 0.0516 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0516 - accuracy: 0.98 - ETA: 0s - loss: 0.0515 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0509 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0514 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0513 - accuracy: 0.98 - ETA: 0s - loss: 0.0512 - accuracy: 0.98 - ETA: 0s - loss: 0.0511 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0510 - accuracy: 0.98 - ETA: 0s - loss: 0.0508 - accuracy: 0.98 - 3s 4ms/step - loss: 0.0508 - accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "history = my_model.fit(training_batches, epochs = EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 1.00 - 0s 234us/sample - loss: 0.0433 - accuracy: 0.9844\n",
      "\n",
      "Loss after training: 0.043\n",
      "Accuracy after training: 98.438%\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    loss, accuracy = my_model.evaluate(image_batch, label_batch)\n",
    "    \n",
    "print(f'\\nLoss after training: {loss:.3f}')\n",
    "print(f'Accuracy after training: {accuracy:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Plot the prediction of the model you created and trained above on a single image from the training set. Also plot the probability predicted by your model for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 236,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_batch, label_batch in training_batches.take(1):\n",
    "    ps = my_model.predict(image_batch)\n",
    "    first_image = image_batch.numpy().squeeze()[0]\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(figsize = (6,9), ncols = 2)\n",
    "ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "ax1.axis('off')\n",
    "ax2.barh(np.arange(10), ps[0], color = 'r', edgecolor = 'k', height = 0.5)\n",
    "ax2.set_aspect(0.1)\n",
    "ax2.set_yticks(np.arange(10))\n",
    "ax2.set_yticklabels(np.arange(10))\n",
    "ax2.set(title = 'Class Probability', xlabel = 'probability', ylabel = 'Class')\n",
    "ax2.set_xlim(0,1.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation  \n",
    "\n",
    "Let's now take a minute to see how TensorFlow calculates and keeps track of the gradients needed for backpropagation. TensorFlow provides a class that records automatic differentiation operations, called `tf.GradientTape`. Automatic differentiation, also known as algorithmic differentiation or simply “autodiff”, is a family of techniques used by computers for efficiently and accurately evaluating derivatives of numeric functions.\n",
    "\n",
    "`tf.GradientTape` works by keeping track of operations performed on tensors that are being \"watched\". By default `tf.GradientTape` will automatically \"watch\" any trainable variables, such as the weights in our model. Trainable variables are those that have `trainable=True`. When we create a model with `tf.keras`, all of the parameters are initialized with `trainable = True`. Any tensor can also be manually \"watched\" by invoking the watch method.\n",
    "\n",
    "\n",
    "Let's see a simple example. Let's take the following equation:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "The derivative of `y` with respect to `x` is given by:\n",
    "\n",
    "$$\n",
    "\\frac{d y}{d x} = 2x\n",
    "$$\n",
    "\n",
    "Now, let's use `tf.GradientTape` to calculate the derivative of a tensor `y` with respect to a tensor `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient calculated by tf.GradientTape: [[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]]\n",
      "\n",
      "\n",
      "True Gradient: [[1.1966898  0.12552415]\n",
      " [0.29263484 0.9696375 ]]\n",
      "\n",
      "\n",
      "Maximum Different: [[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed so things are reproducible\n",
    "tf.random.set_seed(7)\n",
    "\n",
    "# Create a random tensor\n",
    "x = tf.random.normal((2,2))\n",
    "\n",
    "# Calculate the gradient\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x**2\n",
    "    \n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "# Calculate the actual gradient of y = x^2\n",
    "true_grad = 2 * x\n",
    "\n",
    "# print the gradient calculated by tf.GradientTape()\n",
    "print(f'Gradient calculated by tf.GradientTape: {dy_dx}\\n')\n",
    "\n",
    "# print the actual gradient of y = x^2\n",
    "print(f'\\nTrue Gradient: {true_grad}\\n')\n",
    "\n",
    "# print the maximum different between true and calculated gradient\n",
    "print(f'\\nMaximum Different: {np.abs(true_grad - dy_dx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.GradientTape` class keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor.\n",
    "\n",
    "To know more about `tf.GradientTape` and trainable variables check the following links\n",
    "\n",
    "* [Gradient Tape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape)\n",
    "\n",
    "* [TensorFlow Variables](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
