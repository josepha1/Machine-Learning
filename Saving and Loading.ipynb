{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "In this notebook, we'll see how to save and load models with TensorFlow. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 2.1.0\n",
      "\t• tf.keras.version: 2.2.4-tf\n",
      "\t• GPU device not found. Running on CPU\n"
     ]
    }
   ],
   "source": [
    "print('Using:')\n",
    "print(f'\\t\\u2022 TensorFlow version: {tf.__version__}')\n",
    "print(f'\\t\\u2022 tf.keras.version: {tf.keras.__version__}')\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 60\n",
    "test_val_split = 20\n",
    "\n",
    "splits = ['train[:60]', 'train[60:80]', 'train[80:]']\n",
    "\n",
    "dataset, dataset_info = tfds.load('fashion_mnist', split = splits, \n",
    "                                  as_supervised = True, with_info = True)\n",
    "\n",
    "training_set, validation_set, test_set = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42000 images in the testing set\n",
      "There are 14000 images in the validation set\n",
      "There are 14000 images in the test set\n"
     ]
    }
   ],
   "source": [
    "total_examples = dataset_info.splits['train'].num_examples + dataset_info.splits['test'].num_examples\n",
    "\n",
    "num_training_examples = (total_examples * train_split) // 100\n",
    "num_validation_examples = (total_examples * test_val_split) // 100\n",
    "num_test_examples = num_validation_examples\n",
    "\n",
    "print(f'There are {num_training_examples} images in the testing set')\n",
    "print(f'There are {num_validation_examples} images in the validation set')\n",
    "print(f'There are {num_test_examples} images in the test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 263,
       "width": 305
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in training_set.take(1):\n",
    "    image = image.numpy().squeeze()\n",
    "    label = label.numpy()\n",
    "\n",
    "plt.imshow(image, cmap = plt.cm.binary)\n",
    "plt.title(class_names[label])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples // 4).batch(batch_size).map(normalize).prefetch(1)\n",
    "validation_batches = validation_set.cache().batch(batch_size).map(normalize).prefetch(1)\n",
    "testing_batches = test_set.cache().batch(batch_size).map(normalize).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model  \n",
    "\n",
    "Here we'll build and compile our model as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 577,178\n",
      "Trainable params: 577,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer_neurons = [512, 256, 128, 64 ,32, 16]\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28,1)))\n",
    "\n",
    "for neurons in layer_neurons:\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "model.add(tf.keras.layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1/1 [==============================]ss: 2.7250 - accuracy: 0.066 - 1s 1s/step - loss: 2.7250 - accuracy: 0.0667 - val_loss: 2.3097 - val_accuracy: 0.0500\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.5981 - accuracy: 0.1000 - val_loss: 2.2915 - val_accuracy: 0.1500\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.8028 - accuracy: 0.1167 - val_loss: 2.2686 - val_accuracy: 0.1000\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.6131 - accuracy: 0.1333 - val_loss: 2.2738 - val_accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "history = model.fit(training_batches, \n",
    "                    epochs = EPOCHS, \n",
    "                    validation_data = validation_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss for the untrained data is: 2.613\n",
      "The accuracy for the untrained data is: 13.333%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = history.history['loss'][-1], history.history['accuracy'][-1]\n",
    "\n",
    "print(f'The loss for the untrained data is: {loss:.3f}')\n",
    "print(f'The accuracy for the untrained data is: {accuracy:.3%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "In TensorFlow we can save our trained models in different formats. Here we will see how to save our models in TensorFlow's SavedModel format and as HDF5 files, which is the format used by Keras models.\n",
    "\n",
    "### Saving and Loading Models in HDF5 Format\n",
    "\n",
    "To save our models in the format used by Keras models we use the `.save(filepath)` method. For example, to save a model called `my_model` in the current working directory with the name `test_model` we use:\n",
    "\n",
    "```python\n",
    "my_model.save('./test_model.h5')\n",
    "```\n",
    "\n",
    "It's important to note that we have to provide the `.h5` extension to the `filepath` in order the tell `tf.keras` to save our model as an HDF5 file. \n",
    "\n",
    "The above command saves our model into a single HDF5 file that will contain:\n",
    "\n",
    "* The model's architecture.\n",
    "* The model's weight values which were learned during training.\n",
    "* The model's training configuration, which corresponds to the parameters you passed to the `compile` method.\n",
    "* The optimizer and its state. This allows you to resume training exactly where you left off.\n",
    "\n",
    "\n",
    "In the cell below we save our trained `model` as an HDF5 file. The name of our HDF5 will correspond to the current time stamp. This is useful if you are saving many models and want each of them to have a unique name. By default the `.save()` method will **silently** overwrite any existing file at the target location with the same name. If we want `tf.keras` to provide us with a manual prompt to whether overwrite files with the same name, you can set the argument `overwrite=False` in the `.save()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "saved_keras_model_filepath = './{}.h5'.format(int(t))\n",
    "\n",
    "model.save(saved_keras_model_filepath, overwrite = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model has been saved, we can use `tf.keras.models.load_model(filepath)` to re-load our model. This command will also compile our model automatically using the saved training configuration, unless the model was never compiled in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 577,178\n",
      "Trainable params: 577,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reloaded_keras_model = tf.keras.models.load_model(saved_keras_model_filepath)\n",
    "\n",
    "reloaded_keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the re-loaded model has the same architecture as our original model, as it should be. At this point, since we haven't done anything new to the re-loaded model, then both the `reloaded_keras_model` our original `model` should be identical copies. Therefore, they should make the same predictions on the same images. Let's check that this is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in testing_batches.take(1):\n",
    "    prediction_1 = model.predict(image_batch)\n",
    "    prediction_2 = reloaded_keras_model.predict(image_batch)\n",
    "    difference = np.abs(prediction_1 - prediction_2)\n",
    "    print(difference.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the result is 0.0, which indicates that both models made the same predictions on the same images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading TensorFlow SavedModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export our models to the TensorFlow **SavedModel** format, we use the `tf.saved_model.save(model, export_dir)` function. For example, to save a model called `my_model` in a folder called `saved_models` located in the current working directory we use:\n",
    "\n",
    "```python\n",
    "tf.saved_model.save(my_model, './saved_models')\n",
    "```\n",
    "\n",
    "It's important to note that here we have to provide the path to the directory where we want to save our model, **NOT** the name of the file. This is because SavedModels are not saved in a single file. Rather, when you save your model as a SavedModel, `the tf.saved_model.save()` function will create an `assets` folder, a `variables` folder, and a `saved_model.pb` file inside the directory you provided.\n",
    "\n",
    "The SavedModel files that are created contain:\n",
    "\n",
    "* A TensorFlow checkpoint containing the model weights.\n",
    "* A SavedModel proto containing the underlying TensorFlow graph. Separate graphs are saved for prediction (serving), training, and evaluation. If the model wasn't compiled before, then only the inference graph gets exported.\n",
    "* The model's architecture configuration if available.\n",
    "\n",
    "The SavedModel is a standalone serialization format for TensorFlow objects, supported by TensorFlow serving as well as TensorFlow implementations other than Python. It does not require the original model building code to run, which makes it useful for sharing or deploying in different platforms, such as mobile and embedded devices (with TensorFlow Lite), servers (with TensorFlow Serving), and even web browsers (with TensorFlow.js).\n",
    "\n",
    "In the cell below we save our trained model as a SavedModel. The name of the folder where we are going to save our model will correspond to the current time stamp. Again, this is useful if you are saving many models and want each of them to be saved in a unique directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a directory: ./1586562935\\variables; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-475263f4fef5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msavedModel_directory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msavedModel_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m    913\u001b[0m   \u001b[1;31m# the checkpoint, copy assets into the assets directory, and write out the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[1;31m# SavedModel proto itself.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m   \u001b[0mutils_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_or_create_variables_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m   \u001b[0mobject_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m   builder_impl.copy_assets_to_destination_dir(asset_info.asset_filename_map,\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\utils_impl.py\u001b[0m in \u001b[0;36mget_or_create_variables_dir\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    212\u001b[0m   \u001b[0mvariables_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_variables_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mvariables_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m   \"\"\"\n\u001b[1;32m--> 440\u001b[1;33m   \u001b[0mrecursive_create_dir_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m   \"\"\"\n\u001b[1;32m--> 455\u001b[1;33m   \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ./1586562935\\variables; No such file or directory"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "savedModel_directory = './{}'.format(int(t))\n",
    "\n",
    "tf.saved_model.save(model, savedModel_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model has been saved as a SavedModel, we can use `tf.saved_model.load(export_dir)` to re-load our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ./1586562935/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-c7196bfc76ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreloaded_SavedModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavedModel_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(export_dir, tags)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m   \"\"\"\n\u001b[1;32m--> 528\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36mload_internal\u001b[1;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;31m# sequences for nest.flatten, so we put those through as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m   \u001b[0msaved_model_proto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1\n\u001b[0;32m    539\u001b[0m       and saved_model_proto.meta_graphs[0].HasField(\"object_graph_def\")):\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     81\u001b[0m                   (export_dir,\n\u001b[0;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: ./1586562935/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "reloaded_SavedModel = tf.saved_model.load(savedModel_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the object returned by `tf.saved_model.load` is **NOT** a Keras object. Therefore, it doesn't have `.fit`, `.predict`, `.summary`, etc. methods. It is 100% independent of the code that created it. This means that in order to make predictions with our `reloaded_SavedModel` we need to use a different method than the one used with the re-loaded Keras model.\n",
    "\n",
    "To make predictions on a batch of images with a re-loaded SavedModel we have to use:\n",
    "\n",
    "```python\n",
    "reloaded_SavedModel(image_batch, training=False)\n",
    "```\n",
    "\n",
    "This will return a tensor with the predicted label probabilities for each image in the batch. Again, since we haven't done anything new to this re-loaded SavedModel, then both the `reloaded_SavedModel` and our original `model` should be identical copies. Therefore, they should make the same predictions on the same images. Let's check that this is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reloaded_SavedModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-cb4913c345f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtesting_batches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprediction_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprediction_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreloaded_SavedModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdifference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprediction_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reloaded_SavedModel' is not defined"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in testing_batches.take(1):\n",
    "    prediction_1 = model.predict(image_batch)\n",
    "    prediction_2 = reloaded_SavedModel(image_batch, training=False).numpy()\n",
    "    difference = np.abs(prediction_1 - prediction_2)\n",
    "    print(difference.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get back a full Keras model, from a TensorFlow SavedModel, by loading our SavedModel with the `tf.keras.models.load_model` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ./1586562935/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-a10a8717a235>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreloaded_keras_model_from_SavedModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msavedModel_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mreloaded_keras_model_from_SavedModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\josep\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     81\u001b[0m                   (export_dir,\n\u001b[0;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: ./1586562935/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "reloaded_keras_model_from_SavedModel = tf.keras.models.load_model(savedModel_directory)\n",
    "\n",
    "reloaded_keras_model_from_SavedModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models During Training\n",
    "\n",
    "We have seen that when we train a model with a validation set, the value of the validation loss changes through the training process. Since the value of the validation loss is an indicator of how well our model will generalize to new data, it will be great if could save our model at each step of the training process and then only keep the version with the lowest validation loss. \n",
    "\n",
    "We can do this in `tf.keras` by using the following callback:\n",
    "\n",
    "```python\n",
    "tf.keras.callbacks.ModelCheckpoint('./best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "```\n",
    "This callback will save the model as a Keras HDF5 file after every epoch. With the `save_best_only=True` argument, this callback will first check the validation loss of the latest model against the one previously saved. The callback will only save the latest model and overwrite the old one, if the latest model has a lower validation loss than the one previously saved. This will guarantee that will end up with the version of the model that achieved the lowest validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1/1 [==============================]ss: 2.2821 - accuracy: 0.166 - 1s 597ms/step - loss: 2.2821 - accuracy: 0.1667 - val_loss: 2.1259 - val_accuracy: 0.2000\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 1.9897 - accuracy: 0.3167 - val_loss: 1.9512 - val_accuracy: 0.3000\n",
      "Epoch 3/4\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 1.7653 - accuracy: 0.5000 - val_loss: 1.8416 - val_accuracy: 0.3500\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 1.5649 - accuracy: 0.6500 - val_loss: 1.7299 - val_accuracy: 0.3500\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                               monitor = 'val_loss',\n",
    "                                               save_best_only = True)\n",
    "\n",
    "history = model.fit(training_batches, \n",
    "                    epochs = EPOCHS, \n",
    "                    validation_data = validation_batches, \n",
    "                    callbacks = [early_stopping, save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
